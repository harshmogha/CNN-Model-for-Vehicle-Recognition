{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFAw7Fd8yZMr"
      },
      "source": [
        "# **CNN based approach for Vehicle recognising**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Update directory path then any error will not show**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkNOtzJPylj7"
      },
      "source": [
        "# **Create the file structure for the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGkJ0nBLyujz"
      },
      "source": [
        "```\n",
        "vehicle_dataset/\n",
        "â”‚\n",
        "â”œâ”€â”€ vehicle_dataset/\n",
        "â”‚   â”œâ”€â”€ train/\n",
        "â”‚   â”‚   â”œâ”€â”€ cars/\n",
        "â”‚   â”‚   â”œâ”€â”€ trucks/\n",
        "â”‚   â”‚   â”œâ”€â”€ bikes/\n",
        "â”‚   â”‚   â”œâ”€â”€ buses/\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ validation/\n",
        "â”‚   â”‚   â”œâ”€â”€ cars/\n",
        "â”‚   â”‚   â”œâ”€â”€ trucks/\n",
        "â”‚   â”‚   â”œâ”€â”€ bikes/\n",
        "â”‚   â”‚   â”œâ”€â”€ buses/\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ test/\n",
        "â”‚       â”œâ”€â”€ test_image.jpg\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPZrptTyyvZP"
      },
      "source": [
        "store different images in this dataset in train and validation folder according to category\n",
        "\n",
        "warning: avoid space in image name.... use _ or any numebr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **For help**\n",
        "https://youtu.be/K8olahsjyAc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W1O0zoNzBCe"
      },
      "source": [
        "# **now provide the path of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB6ioOG-yYUJ"
      },
      "outputs": [],
      "source": [
        "base_path = \"Path of your dataset\"\n",
        "# update path where you store the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdkmK1i6zPo8"
      },
      "source": [
        "# **Store the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrpMRPbmzVlz"
      },
      "outputs": [],
      "source": [
        "%pip install tensorflow opencv-python matplotlib tensorflow keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWtdRUhXzZ8W"
      },
      "source": [
        "# **import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "voeue_qEzeTg"
      },
      "outputs": [],
      "source": [
        "# Import TensorFlow for deep learning tasks.\n",
        "import tensorflow as tf\n",
        "# Sequential model for stacking layers in order.\n",
        "from tensorflow.keras.models import Sequential\n",
        "# Import CNN layers.\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Explanation of layers:\n",
        "# - Conv2D: Extracts features like edges and textures from images.\n",
        "# - MaxPooling2D: Reduces spatial dimensions, preventing overfitting.\n",
        "# - Flatten: Converts 2D feature maps into a 1D vector before passing to Dense layers.\n",
        "# - Dense: Fully connected layer used for classification.\n",
        "# - Dropout: Randomly disables neurons to prevent overfitting.\n",
        "\n",
        "# For image augmentation (flipping, rotating, zooming, etc.).\\\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# Used for visualizing accuracy, loss, and sample predictions.\n",
        "import matplotlib.pyplot as plt\n",
        "# Handles numerical operations and image data processing.\n",
        "import numpy as np\n",
        "# Helps load and preprocess individual images for predictions.\n",
        "from tensorflow.keras.preprocessing import image\n",
        "# Used for file and directory operations when handling datasets.\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZOoza1N0KBe"
      },
      "source": [
        "## **Define the dataset path where stores the dataset for validation and train means provide path of train and validation folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cedSlCmoziqM"
      },
      "outputs": [],
      "source": [
        "train_dir = \"path of train folder inside the datset\"\n",
        "val_dir = \"path of validation folder inside the datset\"\n",
        "#update path accroding to your dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6sg3QGA0iJq"
      },
      "source": [
        "# **Further process for data preprocessing and Augmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQMzcOeQ0pNj"
      },
      "source": [
        "## Data Preprocessing â†’ Cleaning and transforming raw data to improve model performance.\n",
        "âœ” Handling Missing Data â†’ Fill with mean/median or remove rows.\n",
        "\n",
        "âœ” Feature Scaling â†’ Normalize (0-1) or Standardize (mean=0, std=1).\n",
        "\n",
        "âœ” Encoding Categorical Data â†’ One-Hot or Label Encoding.\n",
        "\n",
        "âœ” Normalization â†’ Min-Max Scaling or Standardization (mean=0, std=1).\n",
        "\n",
        "âœ” Outlier Removal â†’ Z-score or IQR method.\n",
        "\n",
        "âœ” Data Splitting â†’ Train-Test Split (e.g., 80%-20%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCesO5Sa0t5q"
      },
      "source": [
        "## Data Augmentation â†’ Artificially increasing dataset size to improve generalization.\n",
        "âœ” Image Augmentation â†’ Rotation, flipping, zooming, adding noise.\n",
        "\n",
        "âœ” Image Augmentation â†’ Cropping, translation, scaling.\n",
        "\n",
        "âœ” Text Augmentation â†’ Synonym replacement, back translation.\n",
        "\n",
        "âœ” Audio Augmentation â†’ Time stretching, pitch shifting, background noise.\n",
        "\n",
        "ðŸš€ Goal â†’ Prevent overfitting, improve accuracy, and make models robust."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP_JO4mT0wdI"
      },
      "source": [
        "## **data preprocessing and augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZN5q6xCf08Up"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,         # Normalize pixel values to [0,1] range (from [0,255]) for better model performance.\n",
        "    rotation_range=30,      # Randomly rotates images up to 30 degrees to improve generalization.\n",
        "    width_shift_range=0.2,  # Randomly shifts images horizontally by up to 20% of the width.\n",
        "    height_shift_range=0.2, # Randomly shifts images vertically by up to 20% of the height.\n",
        "    shear_range=0.2,        # Applies a shearing transformation to images.\n",
        "    zoom_range=0.2,         # Randomly zooms in/out by up to 20%.\n",
        "    horizontal_flip=True,   # Flips images horizontally to introduce more variability.\n",
        "    fill_mode='nearest'     # Fills missing pixels after transformations using the nearest pixel values.\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)  # Only rescaling is applied for validation data, as no augmentation is needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmI3tybE1Vdg"
      },
      "source": [
        "## **load the image in batches**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqeohUOA1ZkP"
      },
      "outputs": [],
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,              # Path to the training dataset directory.\n",
        "    target_size=(128, 128), # Resizes all images to 128x128 pixels.\n",
        "    batch_size=32,          # Defines the number of images to process in one batch.\n",
        "    class_mode='categorical' # Specifies that the labels are categorical (used for multi-class classification).\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,                # Path to the validation dataset directory.\n",
        "    target_size=(128, 128), # Resizes validation images to match the training size.\n",
        "    batch_size=32,          # Defines batch size for validation.\n",
        "    class_mode='categorical' # Uses categorical labels for multi-class classification.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb7L9k1-2Nyt"
      },
      "source": [
        "# **CNN model for vehicle recognition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og61qcgS2b0f"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    # first convolutional layer: extract the low level features like edges\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),\n",
        "    # reduces spatial size to prevent overfitting\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "\n",
        "    # second convolutional layer: extract more complex patterns\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # third convolutional layer: extract high level features\\\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # converts features maps into a 1D vector for the dense layer\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(128, activation = 'relu'),# fully connected layer to learn patterns\n",
        "    Dropout(0.5), # drop 50% of neurons during traning to prevent overfittingn\n",
        "\n",
        "    # output layer for multiclass classification\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qogBtvo_4D2Q"
      },
      "source": [
        "# **Compile the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnmpI4YX4GDW"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',  # Adam optimizer for adaptive learning rate and efficient training.\n",
        "    loss='categorical_crossentropy',  # Loss function for multi-class classification.\n",
        "    metrics=['accuracy']  # Tracks accuracy during training and evaluation.\n",
        ")\n",
        "\n",
        "\n",
        "# note:\n",
        "'''\n",
        "âœ… optimizer='adam'\n",
        "\n",
        "Adam (Adaptive Moment Estimation) is an advanced gradient descent optimization algorithm.\n",
        "\n",
        "It automatically adjusts learning rates for different parameters, making training efficient.\n",
        "\n",
        "âœ… loss='categorical_crossentropy'\n",
        "\n",
        "Used for multi-class classification where labels are one-hot encoded (e.g., [0, 0, 1, 0]).\n",
        "\n",
        "If labels were integer-encoded, sparse_categorical_crossentropy would be used instead.\n",
        "\n",
        "âœ… metrics=['accuracy']\n",
        "\n",
        "Monitors model performance by calculating accuracy during training and validation.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ3Fa87b4t_7"
      },
      "source": [
        "# **Train model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "nHcmuYlj4xVk",
        "outputId": "3a113b21-562d-42c9-eab0-585c5c7640e4"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_generator, # traning dataset with augumented imaegs\n",
        "    epochs = 20, # number of times model will go through entire traning data\n",
        "    validation_data = val_generator # check model performace\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaCYnXuL5FrF"
      },
      "source": [
        "# **save model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1hmJ-H84589"
      },
      "outputs": [],
      "source": [
        "# model.save(\"C:\\\\Users\\\\harsh\\\\Downloads\\\\vehicle_dataset\\\\vehicle_model.h5\")\n",
        "\n",
        "# both can use but .keras provide future proof\n",
        "\n",
        "model.save(\"path where you store the trained model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGReyX3553FL"
      },
      "source": [
        "# **plot the training history in graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hjeLuX65_GH"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'], label='Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJF4g4K-6CSA"
      },
      "source": [
        "# **test on new image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kc4yXga6JOR"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(\"path of model where you store the trained model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-1kwLov6VKP"
      },
      "source": [
        "# **now test on by using camera**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Spgi3Ds6irO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Load trained model (Update the model path)\n",
        "model_path = \"trained model path\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"Error: Model file not found at {model_path}\")\n",
        "    exit()\n",
        "\n",
        "model = tf.keras.models.load_model(model_path, compile=False)\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Define class labels (Update based on your dataset)\n",
        "class_labels = [\"Car\", \"Truck\", \"Bike\"]  # Modify if needed\n",
        "\n",
        "# Initialize the webcam\n",
        "cap = cv2.VideoCapture(0)  # 0 = Default webcam\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not access the webcam.\")\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    exit()\n",
        "\n",
        "while True:\n",
        "    # Capture frame from webcam\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Error: Failed to capture image.\")\n",
        "        break\n",
        "\n",
        "    # Preprocess the frame for model input\n",
        "    img = cv2.resize(frame, (128, 128))  # Resize to model input size\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions for model input\n",
        "\n",
        "    # Predict the class\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = np.argmax(predictions[0])\n",
        "\n",
        "    # âœ… Fix: Ensure predicted_class is within the valid range\n",
        "    if 0 <= predicted_class < len(class_labels):\n",
        "        predicted_label = class_labels[predicted_class]\n",
        "    else:\n",
        "        predicted_label = \"Unknown\"\n",
        "\n",
        "    # Display prediction on the frame\n",
        "    cv2.putText(frame, f\"Detected: {predicted_label}\", (10, 50),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "    # Show the frame\n",
        "    cv2.imshow(\"Vehicle Detection\", frame)\n",
        "\n",
        "    # Press 'q' to quit\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
